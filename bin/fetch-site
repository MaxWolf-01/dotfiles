#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.8"
# dependencies = []
# ///

"""
Fetch any website and split it into individual markdown pages.

Usage:
    fetch-site <url> [options]
    
Example:
    fetch-site https://example.com/
    fetch-site https://docs.example.com -m "/api/**" -m "/guide/**"
    fetch-site https://blog.example.com --content-selector ".article"
"""

import argparse
import json
import os
import re
import subprocess
import sys
from pathlib import Path
from urllib.parse import urlparse, unquote


def sanitize_filename(url_path: str) -> str:
    """Convert URL path to safe filename."""
    # Decode URL encoding
    url_path = unquote(url_path)
    
    # Remove leading/trailing slashes
    url_path = url_path.strip('/')
    
    # Replace path separators with dashes
    filename = url_path.replace('/', '-')
    
    # Replace non-alphanumeric characters with dashes
    filename = re.sub(r'[^a-zA-Z0-9-]', '-', filename)
    
    # Collapse multiple dashes
    filename = re.sub(r'-+', '-', filename)
    
    # Remove leading/trailing dashes
    filename = filename.strip('-')
    
    return filename or 'index'


def fetch_site(url: str, match_patterns: list = None, concurrency: int = 5, content_selector: str = None) -> str:
    """Run sitefetch command and return the output filename."""
    cmd = ['bunx', 'sitefetch', url, '-o', 'temp_fetch.json']
    
    # Add match patterns
    if match_patterns:
        for pattern in match_patterns:
            cmd.extend(['-m', pattern])
    
    # Add concurrency
    cmd.extend(['--concurrency', str(concurrency)])
    
    # Add content selector if provided
    if content_selector:
        cmd.extend(['--content-selector', content_selector])
    
    print(f"Running: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print(result.stdout)
        return 'temp_fetch.json'
    except subprocess.CalledProcessError as e:
        print(f"Error running sitefetch: {e}")
        print(f"stderr: {e.stderr}")
        sys.exit(1)


def split_pages(json_file: str, output_dir: str):
    """Split fetched JSON into individual markdown pages."""
    # Read the JSON data
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Track created files to handle duplicates
    created_files = {}
    
    print(f"\nSplitting {len(data)} pages into {output_dir}/")
    
    for i, page in enumerate(data):
        # Extract and sanitize filename from URL
        url_path = urlparse(page['url']).path
        base_filename = sanitize_filename(url_path)
        
        # Handle duplicate filenames
        filename = base_filename
        counter = 1
        while filename in created_files:
            filename = f"{base_filename}-{counter}"
            counter += 1
        
        created_files[filename] = True
        
        # Add .md extension
        filename = f"{filename}.md"
        filepath = os.path.join(output_dir, filename)
        
        # Create markdown content
        content = f"""# {page['title']}

**URL:** {page['url']}

---

{page['content']}"""
        
        # Write the file
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"  ✓ {filename}")
    
    # Clean up temp file
    os.remove(json_file)
    
    print(f"\n✅ Successfully split {len(data)} pages into {output_dir}/")


def main():
    parser = argparse.ArgumentParser(
        description='Fetch any website and split it into individual markdown pages',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  fetch-site https://example.com/
  fetch-site https://api.example.com -m "/v1/**" -o my-api-docs
  fetch-site https://blog.example.com --content-selector ".post-content"
        """
    )
    
    parser.add_argument('url', help='URL of the documentation site to fetch')
    parser.add_argument('-o', '--output', help='Output directory name (default: derived from URL)')
    parser.add_argument('-m', '--match', action='append', 
                        help='Match specific pages (can be used multiple times)')
    parser.add_argument('-c', '--concurrency', type=int, default=5,
                        help='Number of concurrent requests (default: 5)')
    parser.add_argument('--content-selector', 
                        help='CSS selector for content extraction')
    
    args = parser.parse_args()

    # Default parent directory
    default_parent = Path.home() / "Documents" / "archive" / "web"

    # Derive output directory from URL if not provided
    if not args.output:
        parsed_url = urlparse(args.url)
        site_name = parsed_url.netloc.replace('.', '-')
        # Remove common prefixes
        site_name = re.sub(r'^(www|docs?)-', '', site_name)
        args.output = str(default_parent / f"{site_name}-docs")
    elif not os.path.isabs(args.output):
        # If user provided relative path, put it in default parent
        args.output = str(default_parent / args.output)
    
    print(f"Fetching documentation from: {args.url}")
    print(f"Output directory: {args.output}")
    
    # Fetch the site
    json_file = fetch_site(
        args.url, 
        args.match, 
        args.concurrency,
        args.content_selector
    )
    
    # Split into pages
    split_pages(json_file, args.output)


if __name__ == '__main__':
    main()
## Credits
# https://github.com/mathiasbynens/dotfiles/blob/main/.functions
# https://github.com/junian/dotfiles/blob/master/.functions
# https://github.com/holman/dotfiles/tree/master/functions
##

# Update dotfiles
dfu() {
  (
    cd ~/.dotfiles && git pull --ff-only && ./install -q
  )
}

# `o` with no arguments opens the current directory, otherwise opens the given location
function o() {
  local target="${1:-.}"
  if [[ -d "$target" ]]; then
    nemo "$target"
  else
    xdg-open "$target"
  fi
}

# Create a directory and cd into it
mcd() {
  mkdir "${1}" && cd "${1}"
}

# Create a .tar.gz archive, using `zopfli`, `pigz` or `gzip` for compression
function targz() {
  local tmpFile="${@%/}.tar"
  tar -cvf "${tmpFile}" --exclude=".DS_Store" "${@}" || return 1

  size=$(
    stat -f"%z" "${tmpFile}" 2>/dev/null # macOS `stat`
    stat -c"%s" "${tmpFile}" 2>/dev/null # GNU `stat`
  )

  local cmd=""
  if ((size < 52428800)) && hash zopfli 2>/dev/null; then
    # the .tar file is smaller than 50 MB and Zopfli is available; use it
    cmd="zopfli"
  else
    if hash pigz 2>/dev/null; then
      cmd="pigz"
    else
      cmd="gzip"
    fi
  fi

  echo "Compressing .tar ($((size / 1000)) kB) using \`${cmd}\`…"
  "${cmd}" -v "${tmpFile}" || return 1
  [ -f "${tmpFile}" ] && rm "${tmpFile}"

  zippedSize=$(
    stat -f"%z" "${tmpFile}.gz" 2>/dev/null # macOS `stat`
    stat -c"%s" "${tmpFile}.gz" 2>/dev/null # GNU `stat`
  )

  echo "${tmpFile}.gz ($((zippedSize / 1000)) kB) created successfully."
}

# Extract archives into a directory named after the archive
# Usage: extract <archive> [destination]
#   - If no destination given, creates dir next to the archive
#   - If archive contains single top-level dir, extracts as-is (no double-wrapping)
extract() {
  local archive="$1"
  local dest="$2"

  if [[ ! -f "$archive" ]]; then
    echo "Error: '$archive' is not a valid file" >&2
    return 1
  fi

  # Get absolute path and base name
  local abs_archive
  abs_archive=$(realpath "$archive")
  local archive_dir=$(dirname "$abs_archive")
  local archive_name=$(basename "$abs_archive")

  # Strip extension(s) for directory name
  local base_name="$archive_name"
  base_name="${base_name%.tar.gz}"
  base_name="${base_name%.tar.bz2}"
  base_name="${base_name%.tar.xz}"
  base_name="${base_name%.tar.zst}"
  base_name="${base_name%.pax.Z}"
  base_name="${base_name%.tar}"
  base_name="${base_name%.zip}"
  base_name="${base_name%.ZIP}"
  base_name="${base_name%.gz}"
  base_name="${base_name%.bz2}"
  base_name="${base_name%.xz}"
  base_name="${base_name%.zst}"
  base_name="${base_name%.rar}"
  base_name="${base_name%.7z}"
  base_name="${base_name%.tgz}"
  base_name="${base_name%.tbz2}"
  base_name="${base_name%.txz}"
  base_name="${base_name%.Z}"

  # Determine extraction destination
  local extract_dir="${dest:-$archive_dir/$base_name}"

  # Create temp dir for extraction
  local tmp_dir=$(mktemp -d)
  trap "rm -rf '$tmp_dir'" EXIT

  echo "Extracting: $archive_name"

  # Extract to temp dir
  case "$archive_name" in
    *.tar.bz2|*.tbz2)  tar -xjf "$abs_archive" -C "$tmp_dir" ;;
    *.tar.gz|*.tgz)    tar -xzf "$abs_archive" -C "$tmp_dir" ;;
    *.tar.xz|*.txz)    tar -xJf "$abs_archive" -C "$tmp_dir" ;;
    *.tar.zst)         tar --zstd -xf "$abs_archive" -C "$tmp_dir" ;;
    *.tar)             tar -xf "$abs_archive" -C "$tmp_dir" ;;
    *.zip|*.ZIP)       unzip -q "$abs_archive" -d "$tmp_dir" ;;
    *.gz)              gunzip -c "$abs_archive" > "$tmp_dir/$base_name" ;;
    *.bz2)             bunzip2 -c "$abs_archive" > "$tmp_dir/$base_name" ;;
    *.xz)              xz -dc "$abs_archive" > "$tmp_dir/$base_name" ;;
    *.zst)             zstd -dc "$abs_archive" > "$tmp_dir/$base_name" ;;
    *.rar)             unrar x -ad "$abs_archive" "$tmp_dir" >/dev/null ;;
    *.7z)              7z x -o"$tmp_dir" "$abs_archive" >/dev/null ;;
    *.pax)             (cd "$tmp_dir" && pax -rf "$abs_archive") ;;
    *.pax.Z)           uncompress -c "$abs_archive" | (cd "$tmp_dir" && pax -r) ;;
    *.Z)               uncompress -c "$abs_archive" > "$tmp_dir/$base_name" ;;
    *)
      echo "Error: Unknown archive format: $archive_name" >&2
      return 1
      ;;
  esac

  if [[ $? -ne 0 ]]; then
    echo "Error: Extraction failed" >&2
    return 1
  fi

  # Check what we got: single top-level dir or multiple items?
  local items=("$tmp_dir"/*(N))
  local item_count=${#items[@]}

  if [[ $item_count -eq 1 && -d "${items[1]}" ]]; then
    # Single directory - move it directly (avoid double-wrapping)
    local single_dir="${items[1]}"
    local final_dest="${dest:-$archive_dir/$(basename "$single_dir")}"

    if [[ -e "$final_dest" ]]; then
      echo "Error: '$final_dest' already exists" >&2
      return 1
    fi

    mv "$single_dir" "$final_dest"
    echo "→ $final_dest/ ($(find "$final_dest" -type f | wc -l) files)"
  else
    # Multiple items or single file - wrap in directory
    if [[ -e "$extract_dir" ]]; then
      echo "Error: '$extract_dir' already exists" >&2
      return 1
    fi

    mkdir -p "$extract_dir"
    mv "$tmp_dir"/* "$extract_dir"
    echo "→ $extract_dir/ ($item_count items)"
  fi

  trap - EXIT
  rm -rf "$tmp_dir"
}

# Determine size of a file or total size of a directory
function fs() {
  if du -b /dev/null >/dev/null 2>&1; then
    local arg=-sbh
  else
    local arg=-sh
  fi
  if [[ -n "$@" ]]; then
    du $arg -- "$@"
  else
    du $arg .[^.]* ./*
  fi
}

# Function to list top n directories by size. If no argument is given, it lists top 10. Negative numbers list smallest n.
lsfs() {
    n=${1:-10} # Default to 10 if no argument is provided
    if (( $n < 0 )); then
        n=$(( -$n ))
        du -ah | sort -h | head -n $n
    else
        du -ah | sort -hr | head -n $n
    fi
}

# Compare original and gzipped file size
function gz() {
  local origsize=$(wc -c <"$1")
  local gzipsize=$(gzip -c "$1" | wc -c)
  local ratio=$(echo "$gzipsize * 100 / $origsize" | bc -l)
  printf "orig: %d bytes\n" "$origsize"
  printf "gzip: %d bytes (%2.2f%%)\n" "$gzipsize" "$ratio"
}

# tre: show a colorized, dirs-first tree including hidden files, excluding .git plus patterns from .gitignore and ~/.gitignore_global; paginated via less
# Options:
#   -e, --exclude PATTERN   Additional pattern to exclude (supports wildcards: *.log, test_*, etc.)
#                           Can be used multiple times: tre -e node_modules -e "*.pyc"
tre() {
  local root; root=$(git rev-parse --show-toplevel 2>/dev/null) || root="$PWD"
  local proj_ign="$root/.gitignore"
  local glob_ign=$(git config --get core.excludesfile | sed "s|^~|$HOME|" || echo "$HOME/.gitignore_global")

  # Parse -e/--exclude options, pass remaining args to tree
  local -a excludes=()
  local -a tree_args=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      -e|--exclude)
        [[ -n "$2" ]] && excludes+=("$2")
        shift 2
        ;;
      --exclude=*)
        excludes+=("${1#--exclude=}")
        shift
        ;;
      -e*)
        excludes+=("${1#-e}")
        shift
        ;;
      *)
        tree_args+=("$1")
        shift
        ;;
    esac
  done

  # Read and clean patterns from gitignore files
  local patterns
  patterns=$(
    grep -hEv '^\s*(#|$)' "$proj_ign" "$glob_ign" 2>/dev/null \
      | sed -E 's@^\./@@; s@/$@@; s@^\*\*/@@' \
      | paste -sd'|' -
  )

  # Build -I argument (always ignore .git)
  if [[ -n $patterns ]]; then
    patterns=".git|$patterns"
  else
    patterns=".git"
  fi

  # Append manual excludes
  for excl in "${excludes[@]}"; do
    patterns="$patterns|$excl"
  done

  tree -aC -I "$patterns" --dirsfirst "${tree_args[@]}" | less -FRNX
}

# TODO, needs fixing: https://github.com/conda/conda/issues/7980
## Disallow installing packages in base environment https://stackoverflow.com/a/69617319/17777085
#function pip() {
#  if [ "${CONDA_PROMPT_MODIFIER-}" = "(base) " ] && [ "$1" = "install" ]; then
#    echo "Not allowed in base"
#  else
#    command pip "$@"
#  fi
#}
#
#function extended_conda() {
#  if [ "${CONDA_PROMPT_MODIFIER-}" = "(base) " ] && [ "$1" = "install" ]; then
#    echo "Not allowed in base"
#  else
#    ~/miniconda/bin/conda "$@"
#  fi
#}
#alias conda=extended_conda
## End of disallow installing packages in base environment

# Change the ssh passphrase of given ssh key
function newsshpwd() {
    echo "Available ssh keys:"
    ls ~/.ssh | grep id_
    ssh-keygen -p -f ~/.ssh/$1
}

# Get the number of files in the current directory
function numel() {
    echo $(ls -1 | wc -l)
}

function count() {
    "$@" | wc -l
}

function choochoo() {
    while true; do
        sl
        clear
    done
}


function tailmail() {
    local user_mail="${1:-max}"
    tac "/var/mail/$user_mail" | sed "/^From /q" | tac
}

### Restic helpers - requires RESTIC_REPOSITORY & RESTIC_PASSWORD_COMMAND to be set

# Show current repo context
function restic_current() {
    echo "Repository: ${RESTIC_REPOSITORY:-not set}"
    echo "Password:   ${RESTIC_PASSWORD_COMMAND:-not set}"
}

# Show diff between snapshots
# Usage:
#   restic_diff              # diff latest 2 snapshots
#   restic_diff <newer> <older>  # diff two snapshots by index (0=latest, 1=2nd latest, ...)
#                                 # or by snapshot ID
#
# Examples:
#   restic_diff              # latest vs 2nd latest
#   restic_diff 0 1          # same as above
#   restic_diff 0 5          # latest vs 6th latest
#   restic_diff abc123 def456  # diff by snapshot IDs
function restic_diff() {
    if [[ -n "$1" && -n "$2" ]]; then
        # Diff between specific snapshots (by index or short_id)
        # Index 0 = latest, 1 = second latest, etc.
        local snap1=$(restic snapshots --json --no-lock | jq -r "reverse | .[$1].short_id // \"$1\"")
        local snap2=$(restic snapshots --json --no-lock | jq -r "reverse | .[$2].short_id // \"$2\"")
        # Always diff from older to newer to show additions
        restic diff --no-lock "$snap2" "$snap1"
    else
        # Default: latest 2
        restic diff --no-lock $(restic snapshots --no-lock --json --latest 2 | jq -r 'map(.short_id) | .[-2:] | join(" ")') "$@"
    fi
}

# Show n largest files in latest snapshot; default 10
function restic_lsfs() {
    local n=${1:-10}
    restic ls --json latest | jq -r 'select(.type == "file") | [.path, .size] | @tsv' | \
        sort -t$'\t' -k2 -n -r | head -n "$n" | awk -F'\t' '{printf "%s\t%.2f MB\n", $1, $2/1048576}'
}

# Show n most recently modified files; default 20
function restic_ls_recent() {
    local n=${1:-20}
    restic ls --json latest | jq -r 'select(.type == "file") | [.mtime, .path] | @tsv' | \
        sort -r | head -n "$n" | column -t -s $'\t'
}

# List all files alphabetically (useful for grep/analysis)
function restic_ls_all() {
    restic ls latest | sort
}

# Show snapshots with compact formatting
function restic_snaps() {
    restic snapshots --compact
}

# put the content of a file into the clipboard, or read from stdin if no file is provided
clip() {
    if [ -z "$1" ]; then
        xclip -selection clipboard
    else
        xclip -selection clipboard < "$1"
    fi
}


# recursively cat the content of a directory (default current)
function rcat() {
    local dir=${1:-.}
    find "$dir" -type f -exec cat {} \;
} 

# recursively flatten a directory structure (defalt current dir), skip files with the same name
function flatten() {
    local dir=${1:-.}
    find "$dir" -mindepth 2 -type f -exec mv -n -t "$dir" {} +
}

# convert a website to markdown and store it to a file in my archive using markdowner
function archive_md() {
    local url="$1"
    local archive_dir="$HOME/Documents/archive/web"
    local filename="${url//[^a-zA-Z0-9]/_}.md"
    #   TODO: optional parameters for subpages, etc.
    curl -s 'https://md.dhr.wtf/?url='"$url" -H 'Content-Type: text/plain' > "$archive_dir/$filename"
    echo "$archive_dir/$filename"
}

# for educational purposes only; use at your own discretion!
# watchy <url>              # default: 2K quality, *marks* sponsors
# watchy <url> best         # best quality available
# watchy <url> fhd sblock   # 1080p max, *removes* sponsors  
# watchy <url> hd noblock   # 720p max, no sponsorblock
watchy() {
    local url="$1"
    local format="bv[height<=1440]+ba/b[height<=1440]"  # Default to 2K
    local sb=('--sponsorblock-mark' 'all')
    local auth=('--cookies-from-browser' 'firefox' '--user-agent' 'Mozilla/5.0')

    # Quality selection  
    [ "$2" = "best" ] && format="bv*+ba/b"
    [ "$2" = "4k" ] && format="bv[height<=2160]+ba/b[height<=2160]"
    [ "$2" = "2k" ] && format="bv[height<=1440]+ba/b[height<=1440]"
    [ "$2" = "fhd" ] && format="bv[height<=1080]+ba/b[height<=1080]"
    [ "$2" = "hd" ] && format="bv[height<=720]+ba/b[height<=720]"
    [ "$2" = "sd" ] && format="bv[height<=480]+ba/b[height<=480]"
    
    # Sponsorblock flags
    [ "$3" = "sblock" ] && sb=('--sponsorblock-remove' 'all')
    [ "$3" = "noblock" ] && sb=()
    
    # Browser flag
    [ -n "$4" ] && auth=('--cookies-from-browser' "$4" '--user-agent' 'Mozilla/5.0')

    yt-dlp "${auth[@]}" "${sb[@]}" -f "$format" \
        -o "$HOME/Downloads/%(title)s.%(ext)s" "$url" && \
        xdg-open "$HOME/Downloads/$(yt-dlp --get-filename -o "%(title)s.%(ext)s" "$url")"
}

latest_notes() {
    local num_notes=${1:-20}
    knb && cd Obsidian/scripts && uv run --script get_fnames.py -m && head filenames.txt -n "$num_notes"
    knb
}

unpublished_notes() {
    local num_notes=${1:-20}
    local whitelist_file="/home/max/repos/obsidian/knowledge-base/whitelist.json"
    local blacklist_file="/home/max/repos/obsidian/knowledge-base/blacklist.txt"
    
    cd ~/repos/obsidian/knowledge-base/Obsidian/scripts
    
    # Generate file list
    uv run --script get_fnames.py -m
    
    # Process in a single pipeline
    sed 's/^"//; s/",$//; s/",$//' filenames.txt | \
    grep -Fxvf <(jq -r '.files[]' "$whitelist_file" 2>/dev/null) | \
    { [[ -f "$blacklist_file" ]] && grep -vf <(grep -v '^#' "$blacklist_file" | grep -v '^[[:space:]]*$') || cat; } | \
    head -n "$num_notes" | \
    while IFS= read -r line; do echo "\"$line\","; done
    
    knb
}

fext() {
    local limit="${1:-50}"
    local dir="${2:-.}"
    
    find "$dir" -type f -exec du -b {} + 2>/dev/null | while read size path; do 
        base=${path:t}
        [[ "$base" =~ '\.' ]] && ext="${base##*.}" || ext="no_ext"
        echo "$size $ext"
    done | awk '{total[$2]+=$1} END {for (e in total) printf "%.0f %s\n", total[e], e}' | \
    sort -rn | head -n "$limit" | while read size ext; do 
        printf "%10s  .%s\n" "$(numfmt --to=iec-i --suffix=B $size)" "$ext"
    done
}

# Delayed command execution using sleep
doin() {
    local time_spec="$1"
    shift
    local cmd="$*"
    
    if [[ -z "$cmd" ]]; then
        echo "Usage: doin <time> <command>"
        echo "Time examples: 10 (seconds), 5m (minutes), 2h (hours)"
        echo "Examples:"
        echo "  doin 5 echo 'Hello after 5 seconds'"
        echo "  doin 1h shutdown -h now"
        return 1
    fi
    
    echo "Scheduling: '$cmd' to run after $time_spec"
    (sleep "$time_spec" && eval "$cmd") &
    local pid=$!
    echo "Running in background (PID: $pid)"
    echo "Use 'kill $pid' to cancel"
}

loadenv() { 
  set -a
  source "${1:-.env}"
  set +a
}

# paper narration helper
# Usage: narrate <arxiv-url-or-local-pdf> [kokoro-args...]
# Saves audio as tts.wav next to the README.
narrate() {
  local src="$1"; shift || true
  local README_PATH DIR TTS_MD OUT
  README_PATH=$(a2m "$src" | tail -n 1)
  [[ -z "$README_PATH" ]] && { echo "Error: Failed to process paper" >&2; return 1; }
  DIR=$(dirname "$README_PATH")
  TTS_MD=$(OPENROUTER_REFERER="https://github.com/MaxWolf-01/dotfiles" \
          OPENROUTER_TITLE="ttsify" \
          uv run --script ~/.dotfiles/bin/ttsify.py "$README_PATH" \
              --model "${TTS_MODEL:-gpt-5-mini}" \
              ${TTS_PROMPT_FILE:+--prompt-file "$TTS_PROMPT_FILE"}) || return 1
  OUT="$DIR/tts.wav"
  (cd "$DIR" && cat "$TTS_MD" | uv run --script "$HOME/bin/kokoro_tts.py" --output "$OUT" "$@") || return 1
}

narrate_this() {
  local file="${1:-README.md}"
  (( $# )) && shift
  local DIR TTS_MD OUT
  [[ ! -f "$file" ]] && { echo "Error: File '$file' not found" >&2; return 1; }
  DIR=$(dirname "$file")
  TTS_MD=$(OPENROUTER_REFERER="https://github.com/MaxWolf-01/dotfiles" \
          OPENROUTER_TITLE="ttsify" \
          uv run --script ~/.dotfiles/bin/ttsify.py "$file" \
              --model "${TTS_MODEL:-gpt-5-mini}" \
              --debug \
              ${TTS_PROMPT_FILE:+--prompt-file "$TTS_PROMPT_FILE"}) || return 1
  OUT="$DIR/tts.wav"
  (cd "$DIR" && cat "$TTS_MD" | uv run --script "$HOME/bin/kokoro_tts.py" --output "$OUT" "$@") || return 1
}

# Shutdown and wake after N hours (default 8) via RTC
sleepfor() {
    local hours=${1:-8}
    sudo rtcwake -m off -s $((hours * 3600))
}

discord() {
    /usr/bin/discord "$@" &
    disown
}

# Update all Claude Code marketplaces and plugins
ccupdate() {
  local dim=$'\e[2m' green=$'\e[32m' cyan=$'\e[36m' red=$'\e[31m' reset=$'\e[0m'

  echo "Updating marketplaces..."
  claude plugin marketplace update
  echo ""
  echo "Updating plugins..."
  local installed_json=~/.claude/plugins/installed_plugins.json
  local plugins=("${(@f)$(jq -r '.plugins | keys[]' "$installed_json" 2>/dev/null)}")
  if [[ ${#plugins[@]} -eq 0 ]]; then
    echo "No installed plugins found"
    return 0
  fi
  local failed=() updated=0
  for plugin in "${plugins[@]}"; do
    [[ -z "$plugin" ]] && continue
    local scope=$(jq -r --arg p "$plugin" '.plugins[$p][0].scope' "$installed_json")
    local output
    output=$(claude plugin update "$plugin" --scope "$scope" 2>&1)
    local ret=$?
    if [[ $ret -ne 0 ]]; then
      echo "  ${red}✗${reset} $plugin ($scope)"
      echo "    ${dim}${output}${reset}"
      failed+=("$plugin")
    elif [[ "$output" == *"already"* ]] || [[ "$output" == *"up to date"* ]] || [[ "$output" == *"up-to-date"* ]]; then
      echo "  ${dim}✓ $plugin ($scope)${reset}"
    else
      echo "  ${cyan}★${reset} ${green}$plugin${reset} ($scope) ${cyan}updated${reset}"
      ((updated++))
    fi
  done
  echo ""
  if [[ ${#failed[@]} -gt 0 ]]; then
    echo "${red}Failed to update: ${failed[*]}${reset}"
    return 1
  fi
  # HACK: restore local dev symlink for mx plugin
  rm -rf ~/.claude/plugins/cache/MaxWolf-01/mx/0.1.0
  ln -s ~/repos/github/MaxWolf-01/agents/memex-workflow ~/.claude/plugins/cache/MaxWolf-01/mx/0.1.0
  if [[ $updated -gt 0 ]]; then
    echo "${green}Updated $updated plugin(s).${reset} Restart Claude Code to apply."
  else
    echo "${dim}All plugins already up to date.${reset}"
  fi
}

